{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfa80ef3-e0de-47e6-b54b-1113f46d691d"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183f37eb-497c-4803-b947-ec9ed1a4adf9"
      },
      "source": [
        "# **Lab: Hyperparameter Tuning with Keras Tuner**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87f0d461-9bfd-49b1-af77-fa98c5cee1de"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97357f65-c520-4ab9-b72a-c34e97431ded"
      },
      "source": [
        "In this lab, you will learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning.\n",
        "\n",
        "## Learning objectives:\n",
        "By the end of this lab, you will:\n",
        "- Install Keras Tuner and import the necessary libraries\n",
        "- Load and preprocess the MNIST data set\n",
        "- Define a model-building function that uses hyperparameters to configure the model architecture\n",
        "- Set up Keras Tuner to search for the best hyperparameter configuration\n",
        "- Retrieve the best hyperparameters from the search and build a model with these optimized values\n",
        "\n",
        "## Prerequisites:\n",
        "- Basic understanding of Python programming\n",
        "- Keras and TensorFlow installed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4d44da5-2935-4a93-879b-baa32249c070"
      },
      "source": [
        "### Exercise 1: Install the Keras Tuner\n",
        "\n",
        "This exercise guides you through the initial setup for using Keras Tuner. You install the library, import necessary modules, and load and preprocess the MNIST data set, which will be used for hyperparameter tuning.\n",
        "1. **Install Keras Tuner:**\n",
        "    - Use pip to install Keras Tuner\n",
        "2. **Import necessary libraries:**\n",
        "    - Import Keras Tuner, TensorFlow, and Keras modules\n",
        "3. **Load and preprocess the MNIST data set:**\n",
        "    - Load the MNIST data set.\n",
        "    - Normalize the data set by dividing by 255.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847a428c-a164-46cb-9d86-7bffa76cce12",
        "outputId": "f693d2f4-c9b0-454d-d6ba-f79391584473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner==1.4.7\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.4.7) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.4.7) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.4.7) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner==1.4.7)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner==1.4.7) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.4.7) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.4.7) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.4.7) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.4.7) (2025.6.15)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner==1.4.7) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner==1.4.7) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner==1.4.7) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner==1.4.7) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow==2.16.2\n",
        "!pip install keras-tuner==1.4.7\n",
        "# !pip install numpy<2.0.0\n",
        "\n"
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae40fb2-7df4-49eb-af6c-ccf40d7c7d93"
      },
      "source": [
        "#### Explanation:\n",
        "This code installs the necessary libraries using pip\n",
        "\n",
        "- **TensorFlow**: Ensures compatibility with the Keras Tuner.\n",
        "- **Keras Tuner**: The version used in this lab.\n",
        "- **Numpy**: Ensures compatibility with the other installed packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60b0d4d6-55ea-42d1-9484-20c278e7ec6f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Increase recursion limit to prevent potential issues\n",
        "sys.setrecursionlimit(100000)"
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fe339e0-00b7-4707-92b8-62b1dc175b1f"
      },
      "source": [
        "#### Explanation:\n",
        "The sys.setrecursionlimit function is used to increase the recursion limit, which helps prevent potential recursion errors when running complex models with deep nested functions or when using certain libraries like TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e9ac456-ffc4-42ed-991f-54d67145f36b"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set TensorFlow log level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter out INFO, 2 = filter out INFO and WARNING, 3 = ERROR only\n",
        "\n"
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd6e5e26-4bb6-4490-993c-2839573f9107",
        "outputId": "89d78c2e-3f01-4bfb-f309-1d4d17e0f5de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_all, y_all), _ = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten and normalize the images\n",
        "x_all = x_all.reshape((x_all.shape[0], -1)).astype(\"float32\") / 255.0\n",
        "\n",
        "# Split into train+val and test (80/20)\n",
        "x_temp, x_test, y_temp, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split train+val into train and validation (75/25 of 80% = 60/20 overall)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e12b4701-7df6-4232-a99d-01cd2b836016"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This code imports the necessary libraries:\n",
        "\n",
        "- **`keras_tuner`**: Used for hyperparameter tuning.\n",
        "- **`Sequential`**: A linear stack of layers in Keras.\n",
        "- **`Dense`**, **`Flatten`**: Common Keras layers.\n",
        "- **`mnist`**: The MNIST dataset, a standard dataset for image classification.\n",
        "- **`Adam`**: An optimizer in Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8b2d6e0-607d-4654-8fc2-ebaa2e29cfb6",
        "outputId": "03089db0-10aa-4776-b5ed-c81584c30b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Validation data shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 3: Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Validation data shape: {x_val.shape}')"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afccd39a-c6a0-4e0b-8e6d-426a3d2e5c33"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This code loads the MNIST dataset and preprocesses it:\n",
        "\n",
        "- **`mnist.load_data()`**: Loads the dataset, returning training and validation splits.\n",
        "- **`x_train / 255.0`**: Normalizes the pixel values to be between 0 and 1.\n",
        "- **`print(f'...')`**: Displays the shapes of the training and validation datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb863c30-436f-4f40-a073-34914d09cd00"
      },
      "source": [
        "### Exercise 2: Defining the model with hyperparameters\n",
        "\n",
        "In this exercise, you define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. This function returns a compiled Keras model that is ready for hyperparameter tuning.\n",
        "\n",
        "**Define a model-building function:**\n",
        "- Create a function `build_model` that takes a `HyperParameters` object as input.\n",
        "- Use the `HyperParameters` object to define the number of units in a dense layer and the learning rate for the optimizer.\n",
        "- Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d10483f1-8b0a-4aef-9f73-ef6826a1dc38"
      },
      "outputs": [],
      "source": [
        "# Define a model-building function\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce4bf167-cd96-487e-8b36-1e7e376e5785"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This function builds and compiles a Keras model with hyperparameters:\n",
        "\n",
        "- **`hp.Int('units', ...)`**: Defines the number of units in the Dense layer as a hyperparameter.\n",
        "- **`hp.Float('learning_rate', ...)`**: Defines the learning rate as a hyperparameter.\n",
        "- **`model.compile()`**: Compiles the model with the Adam optimizer and sparse categorical cross-entropy loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fad9b1-0f80-45b3-88d7-ec6ca7c40496"
      },
      "source": [
        "### Exercise 3: Configuring the hyperparameter search\n",
        "\n",
        "This exercise guides you through configuring Keras Tuner. You create a `RandomSearch` tuner, specifying the model-building function, the optimization objective, the number of trials, and the directory for storing results. The search space summary provides an overview of the hyperparameters being tuned.\n",
        "\n",
        "**Create a RandomSearch Tuner:**\n",
        "- Use the `RandomSearch` class from Keras Tuner.\n",
        "- Specify the model-building function, optimization objective (validation accuracy), number of trials, and directory for storing results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51ce74a-09be-4efa-b96a-c41f7cc5182e",
        "outputId": "f35a6850-ac2a-4855-dc8f-701c522db8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "# Create a RandomSearch Tuner\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Display a summary of the search space\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9828a7aa-e492-489e-b2e4-c74ffb4a51ab"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This code sets up a Keras Tuner `RandomSearch`:\n",
        "\n",
        "- **`build_model`**: The model-building function.\n",
        "- **`objective='val_accuracy'`**: The metric to optimize (validation accuracy).\n",
        "- **`max_trials=10`**: The maximum number of different hyperparameter configurations to try.\n",
        "- **`executions_per_trial=2`**: The number of times to run each configuration.\n",
        "- **`directory='my_dir'`**: Directory to save the results.\n",
        "- **`project_name='intro_to_kt'`**: Name of the project for organizing results.\n",
        "\n",
        "Displays a summary of the hyperparameter search space, providing an overview of the hyperparameters being tuned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f874963e-4716-415d-8801-9e22e3fb7ccd"
      },
      "source": [
        "### Exercise 4: Running the hyperparameter search\n",
        "\n",
        "In this exercise, you run the hyperparameter search using the `search` method of the tuner. You provide the training and validation data along with the number of epochs. After the search is complete, the results summary displays the best hyperparameter configurations found.\n",
        "\n",
        "**Run the search:**\n",
        "- Use the `search` method of the tuner.\n",
        "- Pass in the training data, validation data, and the number of epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f66c076-79ba-4b6b-87ab-99b6b258e33e",
        "outputId": "9cf91c89-669e-45eb-d107-bab74023adbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 01m 16s]\n",
            "val_accuracy: 0.9668000042438507\n",
            "\n",
            "Best val_accuracy So Far: 0.9785999953746796\n",
            "Total elapsed time: 00h 11m 11s\n",
            "Results summary\n",
            "Results in my_dir/intro_to_kt\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.0013842259699198398\n",
            "Score: 0.9785999953746796\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.0024238331032357023\n",
            "Score: 0.9777999818325043\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.0022570414025931407\n",
            "Score: 0.9773499965667725\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.0004251444249324626\n",
            "Score: 0.9771000146865845\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.006038754607580714\n",
            "Score: 0.9708000123500824\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units: 128\n",
            "learning_rate: 0.00030201655847076974\n",
            "Score: 0.9707500040531158\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.007537119294169523\n",
            "Score: 0.9668000042438507\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.00014990131560328783\n",
            "Score: 0.9640500247478485\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.009221840725015822\n",
            "Score: 0.9617500007152557\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.00020537448147820472\n",
            "Score: 0.9433999955654144\n"
          ]
        }
      ],
      "source": [
        "# Run the hyperparameter search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "# Display a summary of the results\n",
        "tuner.results_summary()"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bede143e-8910-4ed9-9c57-f2d0909b4d1a"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This command runs the hyperparameter search:\n",
        "\n",
        "- **`epochs=5`**: Each trial is trained for 5 epochs.\n",
        "- **`validation_data=(x_val, y_val)`**: The validation data to evaluate the model's performance during the search.\n",
        "\n",
        "After the search is complete, this command displays a summary of the best hyperparameter configurations found during the search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a5b1c3-2c01-475f-a514-759b77f789f3"
      },
      "source": [
        "## Exercise 5: Analyzing and using the best hyperparameters\n",
        "\n",
        "In this exercise, you retrieve the best hyperparameters found during the search and print their values. You then build a model with these optimized hyperparameters and train it on the full training data set. Finally, you evaluate the model’s performance on the test set to ensure that it performs well with the selected hyperparameters.\n",
        "\n",
        "**Retrieve the best hyperparameters:**\n",
        "- Use the `get_best_hyperparameters` method to get the best hyperparameters.\n",
        "- Print the optimal values for the hyperparameters.\n",
        "\n",
        "**Build and train the model:**\n",
        "- Build a model using the best hyperparameters.\n",
        "- Train the model on the full training data set and evaluate its performance on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7124821a-7f60-4dca-96f2-754f146d4148",
        "outputId": "8b76732f-660f-4c54-aed8-3c0d4f697d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "The optimal number of units in the first dense layer is 480. \n",
            "\n",
            "The optimal learning rate for the optimizer is 0.0013842259699198398. \n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8906 - loss: 0.3615 - val_accuracy: 0.9698 - val_loss: 0.1046\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9732 - loss: 0.0862 - val_accuracy: 0.9749 - val_loss: 0.0858\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.0527 - val_accuracy: 0.9744 - val_loss: 0.0909\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.0345 - val_accuracy: 0.9756 - val_loss: 0.0841\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0282 - val_accuracy: 0.9764 - val_loss: 0.0873\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0186 - val_accuracy: 0.9789 - val_loss: 0.0831\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9949 - loss: 0.0159 - val_accuracy: 0.9784 - val_loss: 0.0869\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9955 - loss: 0.0138 - val_accuracy: 0.9772 - val_loss: 0.1006\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9957 - loss: 0.0113 - val_accuracy: 0.9776 - val_loss: 0.1042\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9958 - loss: 0.0134 - val_accuracy: 0.9790 - val_loss: 0.0980\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9765 - loss: 0.1088\n",
            "Test accuracy: 0.9811999797821045\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Retrieve the best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"\"\"\n",
        "\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
        "\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Step 2: Build and Train the Model with Best Hyperparameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_val, y_val)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2140dd6-5f9c-4265-a704-cb8a483cf486"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This code retrieves the best hyperparameters found during the search:\n",
        "\n",
        "- **`get_best_hyperparameters(num_trials=1)`**: Gets the best hyperparameter configuration.\n",
        "- **`print(f\"...\")`**: Prints the best hyperparameters.\n",
        "- **`model.fit(...)`**: Trains the model on the full training data with a validation split of 20%.\n",
        "- **`model.evaluate(...)`**: Evaluates the model on the test (validation) dataset and prints the accuracy, which gives an indication of how well the model generalizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6569e85-7b85-44ae-9772-3f551550cc05"
      },
      "source": [
        "## Practice exercises\n",
        "\n",
        "### Exercise 1: Setting Up Keras Tuner\n",
        "\n",
        "#### Objective:\n",
        "Learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning.\n",
        "\n",
        "#### Instructions:\n",
        "1. Install Keras Tuner.\n",
        "2. Import necessary libraries.\n",
        "3. Load and preprocess the MNIST data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ffabf1-adbc-4655-b6b4-5d69c1e3b504",
        "outputId": "a7686b44-e0b0-4f36-ae2d-0a084adde995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Validation data shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# !pip install keras-tuner\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Step 3: Load and preprocess the MNIST data set\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Print the shapes of the training and validation datasets\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Validation data shape: {x_val.shape}')"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5c2e62-43fa-4c44-a052-a6572cbd9e20"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "!pip install keras-tuner\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Step 3: Load and preprocess the MNIST data set\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Print the shapes of the training and validation datasets\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Validation data shape: {x_val.shape}')\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7f458c-7c1c-4660-899c-fcfd02ee92fa"
      },
      "source": [
        "### Exercise 2: Defining the model with hyperparameters\n",
        "\n",
        "#### Objective:\n",
        "Define a model-building function that uses hyperparameters to configure the model architecture.\n",
        "\n",
        "#### Instructions:\n",
        "1. Define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate.\n",
        "2. Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b478a7ce-ff4c-4aaf-a360-0e91cab1e0c1"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Step 1: Define a model-building function\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5410b94-246c-4c25-b9ee-b5f3c0a7ce1d"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Step 1: Define a model-building function\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6275473d-9012-4d64-a4d7-64a301904516"
      },
      "source": [
        "### Exercise 3: Configuring the hyperparameter search\n",
        "\n",
        "#### Objective:\n",
        "Set up Keras Tuner to search for the best hyperparameter configuration.\n",
        "\n",
        "#### Instructions:\n",
        "1. Create a `RandomSearch` tuner using the model-building function.\n",
        "2. Specify the optimization objective, number of trials, and directory for storing results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e381f159-cf43-47a0-bd5e-42cd79232af5",
        "outputId": "b045e645-131c-47e3-d441-13e865b0ee18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from my_dir/intro_to_kt/tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Step 1: Create a RandomSearch Tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,  # Ensure 'build_model' function is defined from previous code\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Display a summary of the search space\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76e241d0-ee32-4e97-837d-35f54a82d5cd"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Step 1: Create a RandomSearch Tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,  # Ensure 'build_model' function is defined from previous code\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Display a summary of the search space\n",
        "tuner.search_space_summary()\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a3e0817-3ab3-4840-b0cd-83ba25170e66"
      },
      "source": [
        "### Exercise 4: Running the hyperparameter search\n",
        "\n",
        "#### Objective:\n",
        "Run the hyperparameter search and dispaly the summary of the results.\n",
        "\n",
        "#### Instructions:\n",
        "1. Run the hyperparameter search using the `search` method of the tuner.\n",
        "2. Pass in the training data, validation data, and the number of epochs.\n",
        "3. Display a summary of the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bd2184b-56e0-4f10-8eab-18e0d151f83e",
        "outputId": "55cddea6-c981-4c34-fe23-29e2294fb956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in my_dir/intro_to_kt\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.0013842259699198398\n",
            "Score: 0.9785999953746796\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.0024238331032357023\n",
            "Score: 0.9777999818325043\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.0022570414025931407\n",
            "Score: 0.9773499965667725\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.0004251444249324626\n",
            "Score: 0.9771000146865845\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.006038754607580714\n",
            "Score: 0.9708000123500824\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units: 128\n",
            "learning_rate: 0.00030201655847076974\n",
            "Score: 0.9707500040531158\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.007537119294169523\n",
            "Score: 0.9668000042438507\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.00014990131560328783\n",
            "Score: 0.9640500247478485\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.009221840725015822\n",
            "Score: 0.9617500007152557\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.00020537448147820472\n",
            "Score: 0.9433999955654144\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Step 1: Run the hyperparameter search\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        " # Display a summary of the results\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c90edacc-d082-4598-93e6-af16f5fc4fa2"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "# Step 1: Run the hyperparameter search\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        " # Display a summary of the results\n",
        "\n",
        "tuner.results_summary()\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4d307e7-3765-4920-a1e1-2cf9b1f38965"
      },
      "source": [
        "### Exercise 5: Analyzing and using the best hyperparameters\n",
        "\n",
        "#### Objective:\n",
        "Retrieve the best hyperparameters from the search and build a model with these optimized values.\n",
        "\n",
        "#### Instructions:\n",
        "1. Retrieve the best hyperparameters using the `get_best_hyperparameters` method.\n",
        "2. Build a model using the best hyperparameters.\n",
        "3. Train the model on the full training data set and evaluate its performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b2f74ec-6bab-4ece-b88b-185ba46a4e9b",
        "outputId": "a10cc74b-c8c6-41ff-e45e-60590b85a5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "The optimal number of units in the first dense layer is 480. \n",
            "\n",
            "The optimal learning rate for the optimizer is 0.0013842259699198398. \n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8953 - loss: 0.3523 - val_accuracy: 0.9672 - val_loss: 0.1048\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9731 - loss: 0.0875 - val_accuracy: 0.9724 - val_loss: 0.0891\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0530 - val_accuracy: 0.9757 - val_loss: 0.0809\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9880 - loss: 0.0369 - val_accuracy: 0.9687 - val_loss: 0.1155\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0283 - val_accuracy: 0.9751 - val_loss: 0.0829\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0213 - val_accuracy: 0.9756 - val_loss: 0.0930\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9943 - loss: 0.0167 - val_accuracy: 0.9734 - val_loss: 0.1124\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9946 - loss: 0.0155 - val_accuracy: 0.9766 - val_loss: 0.1051\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9957 - loss: 0.0122 - val_accuracy: 0.9760 - val_loss: 0.1133\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.0105 - val_accuracy: 0.9777 - val_loss: 0.1044\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9771 - loss: 0.1127\n",
            "Validation accuracy: 0.9801999926567078\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Step 1: Retrieve the best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
        "\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        " # Step 2: Build and train the model with best hyperparameters\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        " # Evaluate the model on the validation set\n",
        "\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "\n",
        "print(f'Validation accuracy: {val_acc}')"
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8180c3e-56be-4fb2-83db-8f8b13617055"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "# Step 1: Retrieve the best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
        "\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        " # Step 2: Build and train the model with best hyperparameters\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        " # Evaluate the model on the validation set\n",
        "\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "\n",
        "print(f'Validation accuracy: {val_acc}')\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ef029a-d153-4f78-be1c-29c20833ce8f"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Congratulations on completing this lab! You have learned to set up Keras Tuner and prepare the environment for hyperparameter tuning. In addition, you defined a model-building function that uses hyperparameters to configure the model architecture. You configured Keras Tuner to search for the best hyperparameter configuration and learned to run the hyperparameter search and analyze the results. Finally, you retrieved the best hyperparameters and built a model with these optimized values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ea0b45d-7294-4c34-9c93-0bd5b89eb859"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd34b5b2-1275-4108-87b4-caeb1f71daeb"
      },
      "source": [
        "Skillup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2636358b-39c1-4787-aa71-06a8a5106d48"
      },
      "source": [
        "Copyright ©IBM Corporation. All rights reserved.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "4480f294d0e5af91350ef1c70e7b3bd8f76b50e8822bcb90342d59ff1810e228",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}