# IBM-AI-Engineer-Specialization

coursera IBM Specialization

## Machine Learning with Python

### module 2

      Simple Regression

#### Lab File [ 1 - Machine Learning with Python/module 2/Simple-Linear-Regression.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%202/Simple-Linear-Regression.ipynb)

      Multiple Linear Regression

#### Lab File [1 - Machine Learning with Python/module 2/Mulitple-Linear-Regression.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%202/Mulitple-Linear-Regression.ipynb)

      Logistic Regression

#### Lab File [1 - Machine Learning with Python/module 2 Logistic Regression/Logistic_Regression.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%202%20Logistic%20Regression/Logistic_Regression.ipynb)

##### Module 2 Summary and Highlights

Congratulations! You have completed this lesson. At this point in the course, you know:

Regression models relationships between a continuous target variable and explanatory features, covering simple and multiple regression types.

Simple regression uses a single independent variable to estimate a dependent variable, while multiple regression involves more than one independent variable.

Regression is widely applicable, from forecasting sales and estimating maintenance costs to predicting rainfall and disease spread.

In simple linear regression, a best-fit line minimizes errors, measured by Mean Squared Error (MSE); this approach is known as Ordinary Least Squares (OLS).

OLS regression is easy to interpret but sensitive to outliers, which can impact accuracy.

Multiple linear regression extends simple linear regression by using multiple variables to predict outcomes and analyze variable relationships.

Adding too many variables can lead to overfitting, so careful variable selection is necessary to build a balanced model.

Nonlinear regression models complex relationships using polynomial, exponential, or logarithmic functions when data does not fit a straight line.

Polynomial regression can fit data but mayoverfit by capturing random noise rather than underlying patterns.

Logistic regression is a probability predictor and binary classifier, suitable for binary targets and assessing feature impact.

Logistic regression minimizes errors using log-loss and optimizes with gradient descent or stochastic gradient descent for efficiency.

Gradient descent is an iterative process to minimize the cost function, which is crucial for training logistic regression models.

### module 3

      Multiclass Classification

#### Lab File [1 - Machine Learning with Python/module 3/Multiclass_Classification.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%203/Multiclass_Classification.ipynb)

      Descision Trees

#### Lab File [1 - Machine Learning with Python/module 3/Decision_Trees.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%203/Decision_Trees.ipynb)

      Regression Trees

#### Lab File [1 - Machine Learning with Python/module 3/Regression_Trees.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%203/Regression_Trees.ipynb)

      Decision Trees and SVM

#### Lab File [1 - Machine Learning with Python/module 3/Decision_Trees_and_SVM.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%203/Decision_Trees_and_SVM.ipynb)

      KNN Classification

#### Lab File [1 - Machine Learning with Python/module 3/KNN_Classification.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%203/KNN_Classification.ipynb)

      Ensamble Learning (Random Forest + XGBoost)

#### Lab File [1 - Machine Learning with Python/module 3/Ensemble_Learning.ipynb](https://github.com/muzammil5539/IBM-AI-Engineer-Specialization/blob/main/1%20-%20Machine%20Learning%20with%20Python/module%203/Ensemble_Learning.ipynb)

##### Module 3 Summary and Highlights

Congratulations! You have completed this lesson. At this point in the course, you know:

Classification is a supervised machine learning method used to predict labels on new data with applications in churn prediction, customer segmentation, loan default prediction, and multiclass drug prescriptions.

Binary classifiers can be extended to multiclass classification using one-versus-all or one-versus-one strategies.

A decision tree classifies data by testing features at each node, branching based on test results, and assigning classes at leaf nodes.

Decision tree training involves selecting features that best split the data and pruning the tree to avoid overfitting.

Information gain and Gini impurity are used to measure the quality of splits in decision trees.

Regression trees are similar to decision trees but predict continuous values by recursively splitting data to maximize information gain.

Mean Squared Error (MSE) is used to measure split quality in regression trees.

K-Nearest Neighbors (k-NN) is a supervised algorithm used for classification and regression by assigning labels based on the closest labeled data points.

To optimize k-NN, test various k values and measure accuracy, considering class distribution and feature relevance.

Support Vector Machines (SVM) build classifiers by finding a hyperplane that maximizes the margin between two classes, effective in high-dimensional spaces but sensitive to noise and large datasets.

The bias-variance tradeoff affects model accuracy, and methods such as bagging, boosting, and random forests help manage bias and variance to improve model performance.

Random forests use bagging to train multiple decision trees on bootstrapped data, improving accuracy by reducing variance.
